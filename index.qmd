---
title: "BatReportQuarto"
---

# INTRODUCTION

A simple bat report from minimal data demonstrating literate programming with Quarto (or the world beyond Excel).

## Install R RStudio and Packages  

1. Download and install the latest version of R  <https://cran.r-project.org/bin/windows/base/>. Download the version for your operating system; R can be downloaded for Windows, Mac & Linux.    
2. It is recommended R is used through the _RStudio_ IDE. Download and install the latest version of RStudio from their web page <https://www.rstudio.com/products/rstudio/#Desktop>.  Download the _free_ desktop version.

### Install the _iBats_ Package from GitHub

The `iBats` package contains functions that help with the _Data Science_ of bat survey results. To install this package use the code below in the console (see Figure ); _one line at a time_. The package is installed from GitHub.   


```{r} 
#| echo=TRUE, 
#| eval=FALSE

install.packages("devtools")

devtools::install_github("Nattereri/iBats")


```


# TIDY DATA

_Tidy Data_ is a consistent way to organise your data [@wickham_tidy_2014]. Getting your data into this format requires some initial work, but that effort pays off in the long term. Once you have tidy data you will spend _less_ time wrangling data from one representation to another, allowing you to spend more time on the analytic questions at hand. Unfortunately, there is a rule of thumb; 80% of time doing data science is spent wrangling data; particularly the effort required in sorting and rearranging the data into the _tidy_ and therefore usable format. 

There are three interrelated rules which make a dataset tidy, such as the data shown in Table \@ref(tab:table01):

* Each variable must have its own column.
* Each observation must have its own row.
* Each value must have its own cell.

![](Images/TidyDataBats.PNG)

## Minimal Data Requirement

To undertake meaningful data analysis, it is recommended that data collected from  bat activity surveys is wrangled into _tidy data_ that has the following five variables (columns) as a minimum: 

* Description
* DateTime
* Species
* Latitude
* Longitude

The rationale for these variables is as follows:

`Description` a column to help identify the observation for example a location, surveyor or survey number.


`DateTime`: the date and time of the bat observation to  BS ISO 8601:2004 i.e. `yyyymmdd hh:mm:ss`. The use of BS ISO 8601:2004 prevents confusion over the date format ^[the standard is recommended by .gov.uk<https://www.gov.uk/government/publications/open-standards-for-government/date-times-and-time-stamps-standard>] .  Reference bat activity to the local time and specifying an _iana_^[a full list of time zones can be found here <https://en.wikipedia.org/wiki/List_of_tz_database_time_zones>] time zone allows for daylight saving times to considered; the _iana_ code for the UK is `Europe/London`.

`Species`: bat species names should follow the "binomial nomenclature" from the International Code of Zoological Nomenclature (ICZN)^[<https://www.iczn.org/the-code/the-international-code-of-zoological-nomenclature/the-code-online/>] - e.g. _Barbastella barbastellus_, _Eptesicus serotinus_, etc... A column of local common names can always be added to the _tidy_ data, i.e. in a separate column see Appendix 8. Sound analysis may not be able to distinguish calls to species level; in practice some calls may only be identified to genus or less; Table  \@ref(tab:table04) gives a practical and consistent guide to naming^[Adapted from the Ecobat guide.].

`Longitude` and `Latitude`: World Geodetic System 1984^[ <https://en.wikipedia.org/wiki/World_Geodetic_System>] (WGS84); as used by Google earth. A digital, numeric, format should be used. Any other spatial reference system can be used (e.g. British National Grid Easting/Northing), as this can be stored as an extra column in the _tidy data_, the prerequisite is that the reference system can be converted to WGS84; which is the case for most national or state co-ordinate systems. Using a global co-ordinate system such as WSG84 give access to the many open-source application programming interfaces (API) available that assist with data analysis (e.g. assessing sunset and sunrise times or the adjustment to daylight saving). 